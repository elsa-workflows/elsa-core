receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    send_batch_max_size: 100
    send_batch_size: 10  # Increased batch size for efficiency
    timeout: 1s
  tail_sampling:
    decision_wait: 10s
    num_traces: 10000  # Increased from 100 to handle more traces
    expected_new_traces_per_sec: 100  # Increased from 10
    decision_cache:
      sampled_cache_size: 100000
    policies: [
      {
        name: incidents-policy,
        type: boolean_attribute,
        boolean_attribute: { key: hasIncidents, value: true }
      }
    ]

exporters:
  debug:
    verbosity: detailed
  datadog:
    api:
      site: ${env:DD_SITE}
      key: ${env:DD_API_KEY}

service:
  pipelines:
    metrics:
      receivers: [ otlp ]
      processors: [ batch ]
      exporters: [ datadog ]
    traces:
      receivers: [ otlp ]
      processors: [ batch, tail_sampling ]  # Added tail_sampling to the main traces pipeline
      exporters: [ debug, datadog ]  # Directly exporting to debug and datadog
    logs:
      receivers: [ otlp ]
      processors: [ batch ]
      exporters: [ debug, datadog ]
